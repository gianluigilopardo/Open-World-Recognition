{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ICaRLMain.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLMLAOPyrR1K"
      },
      "source": [
        "# Import GitHub repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LqwZJLUlcYH"
      },
      "source": [
        "import os\n",
        "import logging\n",
        "import sys"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVa_FlnxrXIk"
      },
      "source": [
        "if not os.path.isdir('./owr'):\n",
        "  !git clone -b icarl https://github.com/gianluigilopardo/Open-World-Recognition.git\n",
        "  !mv 'Open-World-Recognition' 'owr'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXQyHMXzrZ5A"
      },
      "source": [
        "# Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fbc5JEp-MIc2"
      },
      "source": [
        "from owr import ResNet\n",
        "from owr.dataset import Subset\n",
        "from owr.icarl import classify\n",
        "from owr.icarl import incremental_train\n",
        "from owr.icarl import update_representation\n",
        "from owr.icarl import construct_exemplar_set\n",
        "from owr.icarl import reduce_exemplars\n",
        "from owr.icarl import generate_new_exemplars\n",
        "from owr.models import compute_loss\n",
        "from owr.models import train_network\n",
        "from owr import params\n",
        "from owr.utils import get_classes_names\n",
        "from owr.utils import get_task_indexes\n",
        "from owr.utils import splitter\n",
        "from owr.utils import map_splits\n",
        "from owr.utils import get_classes\n",
        "from owr.utils import get_indexes\n",
        "import owr.models"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PG5dRGuxOMtB"
      },
      "source": [
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn import functional as F\n",
        "import random"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2rkcBbIKfUQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6feded2-aba6-49c2-9d76-66b88241d0e9"
      },
      "source": [
        "print(params.SEED)\n",
        "print(params.NUM_WORKERS)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "42\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FlAkShyryrf"
      },
      "source": [
        "# Define Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AP2iR2vl3Wiz"
      },
      "source": [
        "train_transformer = transforms.Compose([transforms.RandomCrop(size = 32, padding=4),\n",
        "                                         transforms.RandomHorizontalFlip(),\n",
        "                                         transforms.ToTensor(),\n",
        "                                         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "test_transformer = transforms.Compose([transforms.ToTensor(),\n",
        "                                         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CSNk0NlrvAL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fe85a72-208b-4826-d4bf-c675571953d7"
      },
      "source": [
        "from torchvision import datasets\n",
        "trainDS = datasets.cifar.CIFAR100( 'data', train=True, download=True, transform=train_transformer)\n",
        "testDS = datasets.cifar.CIFAR100( 'data', train=False, download=True, transform=test_transformer)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3ge3VayryJL"
      },
      "source": [
        "splits = splitter()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzxTlFF_rkfe"
      },
      "source": [
        "# Define Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgZtPkiPrmQK"
      },
      "source": [
        "ICaRL = ResNet.resnet32(num_classes=100)\n",
        "ICaRL =  ICaRL.to(params.DEVICE)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CI8EyFmpOikN"
      },
      "source": [
        "exemplars = [None]*params.NUM_CLASSES\n",
        "\n",
        "test_indexes =  []\n",
        "accs = []\n",
        "classifier = 'nme'"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5CCzkSlGC1l"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvRh1OcsF3Ux"
      },
      "source": [
        "def print_stats(preds, true, col):\n",
        "  preds_old_classes = 0\n",
        "  preds_new_classes = 0\n",
        "  true_new_classes = 0\n",
        "  true_old_classes = 0\n",
        "\n",
        "  for cl in col[:-10]:\n",
        "    mapped_class = map_splits([cl], col)\n",
        "    for predicted in preds:\n",
        "      if mapped_class == predicted:\n",
        "        preds_old_classes += 1\n",
        "    for actual in true:\n",
        "      if mapped_class == actual:\n",
        "        true_old_classes += 1\n",
        "\n",
        "  for cl in col[-10:]:\n",
        "    mapped_class = map_splits([cl], col)\n",
        "    for predicted in preds:\n",
        "      if mapped_class == predicted:\n",
        "        preds_new_classes += 1\n",
        "    for actual in true:\n",
        "      if mapped_class == actual:\n",
        "        true_new_classes += 1\n",
        "\n",
        "  print('Tot predictions new classes:', preds_new_classes, 'over a total of', true_new_classes)\n",
        "  print('Tot predictions old classes:', preds_old_classes, 'over a total of', true_old_classes )\n",
        "  return"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcFjbBGrOMz6"
      },
      "source": [
        "for task in range(0, params.NUM_TASKS*params.TASK_SIZE, params.TASK_SIZE):\n",
        "    train_indexes = get_task_indexes(trainDS, task)\n",
        "    test_indexes = test_indexes + get_task_indexes(testDS, task)\n",
        "\n",
        "    train_dataset = Subset(trainDS, train_indexes, transform = train_transformer)\n",
        "    test_dataset = Subset(testDS, test_indexes, transform = test_transformer)\n",
        "\n",
        "    train_loader = DataLoader( train_dataset, num_workers=2, batch_size=params.BATCH_SIZE, shuffle=True)\n",
        "    test_loader = DataLoader( test_dataset, num_workers=2, batch_size=params.BATCH_SIZE , shuffle=True )\n",
        "    print(task)\n",
        "    ICaRL, exemplars = incremental_train(trainDS, ICaRL, exemplars, task, train_transformer)\n",
        "    col = []\n",
        "    for i,x in enumerate( splits[ :int(task/10) + 1]) : \n",
        "        v = np.array(x)\n",
        "        col = np.concatenate( (col,v), axis = None)\n",
        "        col = col.astype(int)\n",
        "    mean = None\n",
        "    total = 0.0\n",
        "    running_corrects = 0.0\n",
        "    \n",
        "    for img, lbl, _ in train_loader:\n",
        "        img = img.float().to(params.DEVICE)\n",
        "        preds, mean = classify(img, exemplars, ICaRL, task, trainDS, mean)\n",
        "        preds = preds.to(params.DEVICE)\n",
        "        labels = map_splits(lbl, col).to(params.DEVICE)\n",
        "        total += len(lbl)\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "    accuracy = float(running_corrects/total)\n",
        "    print(f'task: {task}', f'train accuracy = {accuracy}')\n",
        "\n",
        "    total = 0.0\n",
        "    running_corrects = 0.0\n",
        "    tot_preds = []\n",
        "    tot_lab = []\n",
        "    for img, lbl, _ in test_loader:\n",
        "        img = img.float().to(params.DEVICE)\n",
        "        preds, _ = classify(img, exemplars, ICaRL, task, trainDS, mean)\n",
        "        preds = preds.to(params.DEVICE)\n",
        "        labels = map_splits(lbl, col).to(params.DEVICE)\n",
        "        tot_preds = np.concatenate( ( tot_preds, preds.data.cpu().numpy() ) )\n",
        "        tot_lab = np.concatenate( (tot_lab, labels.data.cpu().numpy()  ) )\n",
        "        total += len(lbl)\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "    print_stats(tot_preds, tot_lab, col)\n",
        "\n",
        "    accuracy = float(running_corrects/total)\n",
        "    print(f'task: {task}', f'test accuracy = {accuracy}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}