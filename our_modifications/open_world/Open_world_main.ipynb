{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Open_world_main.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNHonjXfBvYyBjTMpEXuXJi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gianluigilopardo/Open-World-Recognition/blob/main/our_modifications/open_world/Open_world_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjBxkRrfQzn8",
        "outputId": "01c9d687-fdd3-4247-9a06-2f610ec4172b"
      },
      "source": [
        "import os\n",
        "import logging\n",
        "import sys\n",
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wr4LwmKwRot-"
      },
      "source": [
        "if not os.path.isdir('./owr'):\n",
        "  !git clone https://github.com/gianluigilopardo/Open-World-Recognition.git\n",
        "  !mv 'Open-World-Recognition' 'owr'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "id": "FxMSpIJeR046",
        "outputId": "bf9372c9-27af-4ce7-e168-829e2c83553a"
      },
      "source": [
        "import copy\n",
        "import os\n",
        "import logging\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision import datasets\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sn\n",
        "import torch.nn as nn\n",
        "\n",
        "from owr.our_modifications.open_world import BiC\n",
        "from owr.our_modifications.open_world import ResNet\n",
        "from owr.our_modifications.open_world import models\n",
        "from owr.our_modifications.open_world import params\n",
        "from owr.our_modifications.open_world import utils\n",
        "from owr.our_modifications.open_world.dataset import *\n",
        "from owr.our_modifications.open_world import ThresholdsLearner #va caricato nel branch main\n",
        "from collections import defaultdict\n",
        "\n",
        "# This script is the main for running the class-specific learnd rejection strategy for BiC method.\n",
        "# remeber to set lr = 0.1 in params before running BiC.\n",
        "\n",
        "### Useful functions for compute evaluation metrics ###\n",
        "\n",
        "def return0dot0():\n",
        "    return 0.0\n",
        "def returnList():\n",
        "    return []\n",
        "def harmonic_mean(a,b):\n",
        "    return (2 * a * b) / (a + b)\n",
        "\n",
        "soft_max = nn.Softmax(dim=1)\n",
        "\n",
        "print(\"BiC owr - Data Learned Thresholds\")\n",
        "print(f\"learning rate : {params.LR}\")\n",
        "print(f\"learning rate schedule epochs: {params.STEP_SIZE}\")\n",
        "\n",
        "\n",
        "\n",
        "############################################################\n",
        "#################### DATA MANAGEMENT #######################\n",
        "\n",
        "cifar = datasets.cifar.CIFAR100\n",
        "# transformers\n",
        "train_transformer = transforms.Compose([transforms.RandomCrop(size=32, padding=4),\n",
        "                                        transforms.RandomHorizontalFlip(),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                                        ])\n",
        "\n",
        "test_transformer = transforms.Compose([transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                                       ])\n",
        "train_dataset = cifar('data', train=True, download=True, transform=train_transformer)\n",
        "test_dataset = cifar('data', train=False, download=True, transform=test_transformer)\n",
        "# get the incremental subdivision of classes - Inside the function there is a seed that can be changed\n",
        "# in order to evaluate another class sequence\n",
        "# splits\n",
        "splits = utils.splitter()\n",
        "print('splits: ' + str(splits))\n",
        "\n",
        "# Get the open_world_test_indexes, i.e. test set for evaluate the capability of reject\n",
        "open_world_test_indexes = [] # list of indexes\n",
        "for task in range(params.NUM_CLASSES//2,params.NUM_CLASSES, params.TASK_SIZE):\n",
        "    open_world_test_indexes = open_world_test_indexes + utils.get_task_indexes(test_dataset, task)\n",
        "open_world_test_subset = Subset(test_dataset, open_world_test_indexes, transform=test_transformer)\n",
        "open_word_test_loader = DataLoader(open_world_test_subset, num_workers=params.NUM_WORKERS,\n",
        "                                     batch_size=params.BATCH_SIZE, shuffle=True)\n",
        "# this test set contains only unknown classes for our purpose\n",
        "closed_word_test_indexes = [] # test set for closed world with and without rejection\n",
        "\n",
        "###################################################################################\n",
        "##################### instantiate the model's object ##############################\n",
        "\n",
        "# The following for BiC\n",
        "model = BiC.BiC_method(num_classes=params.NUM_CLASSES).to(params.DEVICE)\n",
        "\n",
        "############################################################################\n",
        "##### lists for the evaluation phase\n",
        "# They will store the accuracy curves\n",
        "# we do not need dict---> There is only a (vector of) threshold => a list for saving results is enough\n",
        "closed_word_without_rejection_accuracy = []\n",
        "closed_word_with_rejection_accuracy = []\n",
        "open_set_accuracy = []\n",
        "open_world_aritmetic_mean = []\n",
        "open_world_harmonic_mean = []\n",
        "\n",
        "unknown_label = -1\n",
        "\n",
        "for task in range(0, params.NUM_CLASSES // 2, params.TASK_SIZE):\n",
        "    not_known = 0 ## for compute a statistics\n",
        "    ################################################################################################\n",
        "    ############### INSTANTIATE THE THRESHOLD PARAMETER FOR THIS TASK ##############################\n",
        "    threshold = nn.parameter.Parameter(torch.ones(task + params.TASK_SIZE, device=params.DEVICE))\n",
        "\n",
        "    ############################ Data employed in this task ########################################\n",
        "    # Train and Test datasets for this tasks\n",
        "    train_indexes = utils.get_task_indexes(train_dataset, task)\n",
        "    closed_word_test_indexes = closed_word_test_indexes + utils.get_task_indexes(test_dataset, task)\n",
        "    new_test_indexes = utils.get_task_indexes(test_dataset, task)\n",
        "\n",
        "    train_subset = Subset(train_dataset, train_indexes, transform=train_transformer)\n",
        "    test_subset = Subset(test_dataset, closed_word_test_indexes, transform=test_transformer)\n",
        "    new_test_subset = Subset(test_dataset, new_test_indexes, transform=test_transformer)\n",
        "\n",
        "    train_loader = DataLoader(train_subset, num_workers=params.NUM_WORKERS,\n",
        "                              batch_size=params.BATCH_SIZE, shuffle=True)\n",
        "    closed_word_test_loader = DataLoader(test_subset, num_workers=params.NUM_WORKERS,\n",
        "                             batch_size=params.BATCH_SIZE, shuffle=True)\n",
        "    new_test_loader = DataLoader(new_test_subset, num_workers=params.NUM_WORKERS,\n",
        "                             batch_size=params.BATCH_SIZE, shuffle=True)\n",
        "    ############################################################################################\n",
        "    ############################# INCREMENTAL TRAINING  #######################################\n",
        "    _, _, threshold_dataset_loader = model.incremental_training(train_dataset, train_transformer, task, new_test_loader,\n",
        "                                                                closed_word_test_loader, with_rejection= True)\n",
        "    ############################################################################################\n",
        "    ############################# LEARNING THE THRESHOLDS ######################################\n",
        "    # currently discovered classes\n",
        "    classes = []\n",
        "    for i, x in enumerate(splits[:int(task / params.TASK_SIZE) + 1]):\n",
        "        v = np.array(x)\n",
        "        classes = np.concatenate((classes, v), axis=None)\n",
        "        classes = classes.astype(int)\n",
        "    threshold = ThresholdsLearner.learn_thresholds(threshold, threshold_dataset_loader, classes, model, task)\n",
        "    print('\\n The learned threeshold are : ')\n",
        "    print(threshold)\n",
        "    ################################### Evaluation ####################################################\n",
        "    with torch.no_grad():\n",
        "        ############## Closed word without rejection\n",
        "        total = 0.0\n",
        "        running_corrects = 0.0\n",
        "        not_known = 0\n",
        "        batch = 1\n",
        "\n",
        "        for img, lbl, _ in closed_word_test_loader:\n",
        "            img = img.float().to(params.DEVICE)\n",
        "\n",
        "            outputs = model(img, task).to(params.DEVICE)\n",
        "            cut_outputs = np.take_along_axis(outputs.to(params.DEVICE), classes[None, :], axis=1).to(params.DEVICE)\n",
        "            probs = soft_max(cut_outputs)\n",
        "\n",
        "            _, preds = torch.max(probs.data, 1)\n",
        "\n",
        "            labels = utils.map_splits(lbl, classes).to(params.DEVICE)\n",
        "            total += len(lbl)\n",
        "\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "            batch = batch + 1\n",
        "\n",
        "        accuracy = float(running_corrects / total)\n",
        "        closed_word_without_rejection_accuracy.append(accuracy)\n",
        "        print(f'\\n task: {task}', f'closed_word_without_rejection_accuracy = {accuracy}')\n",
        "\n",
        "        ############## Closed word with rejection\n",
        "        # use the rejection\n",
        "        total = 0.0\n",
        "        running_corrects = 0.0\n",
        "        not_known = 0\n",
        "        batch = 1\n",
        "        running_corrects_dict = 0.0\n",
        "\n",
        "        for img, lbl, _ in closed_word_test_loader:\n",
        "            img = img.float().to(params.DEVICE)\n",
        "\n",
        "            outputs = model(img, task).to(params.DEVICE)\n",
        "            cut_outputs = np.take_along_axis(outputs.to(params.DEVICE), classes[None, :], axis=1).to(params.DEVICE)\n",
        "            probs = soft_max(cut_outputs)\n",
        "\n",
        "            # maxPred, preds = torch.max(probs.data, 1)\n",
        "            labels = utils.map_splits(lbl, classes).to(params.DEVICE)\n",
        "            total += len(lbl)\n",
        "            # Apply the rejection strategy with a method\n",
        "            preds = ThresholdsLearner.rejection(probs, threshold)\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "            batch = batch + 1\n",
        "        accuracy = float(running_corrects / total)\n",
        "        closed_word_with_rejection_accuracy.append(accuracy)\n",
        "        print(f'task: {task}' f'closed_word_with_rejection_accuracy = {accuracy}')\n",
        "\n",
        "        ############################### Open Word Scenario\n",
        "        # use the rejection\n",
        "        total = 0.0\n",
        "        running_corrects = 0.0\n",
        "        not_known = 0\n",
        "        batch = 1\n",
        "        running_corrects_dict = 0.0\n",
        "\n",
        "        for img, _ , _ in open_word_test_loader:\n",
        "            # we do not need labels, this are only unknown\n",
        "            img = img.float().to(params.DEVICE)\n",
        "\n",
        "            outputs = model(img, task).to(params.DEVICE)\n",
        "            cut_outputs = np.take_along_axis(outputs.to(params.DEVICE), classes[None, :], axis=1).to(params.DEVICE)\n",
        "            probs = soft_max(cut_outputs)\n",
        "\n",
        "            # maxPred, preds = torch.max(probs.data, 1)\n",
        "            total += len(preds)\n",
        "            # Apply the rejection strategy with a method\n",
        "            preds = ThresholdsLearner.rejection(probs, threshold)\n",
        "            running_corrects += torch.sum(preds == unknown_label).data.item()\n",
        "        accuracy = float(running_corrects / total)\n",
        "        open_set_accuracy.append(accuracy)\n",
        "        print(f'task: {task}' f', open_set_accuracy = {accuracy}')\n",
        "\n",
        "#### THE INCREMENTAL TRAINING HAS ENDED ####\n",
        "############## Compute Open World Harmonic and Aritmetic Mean\n",
        "open_world_aritmetic_mean = [(a + b)/2 for a,b in zip(closed_word_with_rejection_accuracy, open_set_accuracy)]\n",
        "open_world_harmonic_mean = [harmonic_mean(a,b) for a,b in zip(closed_word_with_rejection_accuracy, open_set_accuracy)]\n",
        "\n",
        "##########################################################################################\n",
        "############################ PRINT FINAL RESULTS #########################################\n",
        "print(\"\\n RESULTS : \")\n",
        "\n",
        "print(\"\\n Closed world without rejection accuracy\")\n",
        "print(closed_word_without_rejection_accuracy)\n",
        "\n",
        "print(\"\\n Closed world with rejection accuracy\")\n",
        "print(closed_word_with_rejection_accuracy)\n",
        "\n",
        "print(\"\\n Open world accuracy\")\n",
        "print(open_set_accuracy)\n",
        "\n",
        "print(\"\\n Open world aritmetic mean\")\n",
        "print(open_world_aritmetic_mean)\n",
        "\n",
        "print(\"\\n Open world harmonic mean\")\n",
        "print(open_world_harmonic_mean)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BiC owr - Data Learned Thresholds\n",
            "learning rate : 0.1\n",
            "learning rate schedule epochs: [49, 63]\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "splits: [[81, 14, 3, 94, 35, 31, 28, 17, 13, 86], [90, 18, 4, 42, 38, 34, 21, 16, 96, 76], [22, 5, 49, 45, 41, 25, 20, 85, 15, 68], [27, 6, 57, 53, 50, 32, 26, 65, 70, 82], [72, 11, 1, 80, 39, 36, 33, 12, 95, 10], [84, 24, 2, 51, 47, 46, 29, 23, 74, 19], [43, 7, 61, 59, 58, 44, 40, 37, 77, 98], [79, 30, 0, 88, 56, 55, 89, 48, 97, 73], [54, 8, 66, 64, 91, 52, 71, 9, 69, 92], [67, 99, 83, 63, 60, 87, 62, 75, 78, 93]]\n",
            "\n",
            "FIRST STAGE OF TRAINING, Task : 0\n",
            "\n",
            "Step: 0, Epoch: 0, Loss: 2.3941743506325617, Accuracy: 0.20266666666666666\n",
            "Step: 0, Epoch: 1, Loss: 1.8264003528489008, Accuracy: 0.3502222222222222\n",
            "Step: 0, Epoch: 2, Loss: 1.6412094699011908, Accuracy: 0.4111111111111111\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-0b080a0ea050>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;31m############################# INCREMENTAL TRAINING  #######################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     _, _, threshold_dataset_loader = model.incremental_training(train_dataset, train_transformer, task, new_test_loader,\n\u001b[0;32m--> 120\u001b[0;31m                                                                 closed_word_test_loader, with_rejection= True)\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0;31m############################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;31m############################# LEARNING THE THRESHOLDS ######################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/owr/our_modifications/open_world/BiC.py\u001b[0m in \u001b[0;36mincremental_training\u001b[0;34m(self, train_dataset, train_transformer, task, new_test_loader, test_loader, with_rejection)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nFIRST STAGE OF TRAINING, Task : {task}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;31m# first stage training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mloss_curve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_accs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_stage_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_splits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;31m# before performing the bias correction we evaluate the baseline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/owr/our_modifications/open_world/BiC.py\u001b[0m in \u001b[0;36mfirst_stage_training\u001b[0;34m(self, data_loader, classes, task, train_splits)\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0;31m# features=False : use fully connected layer (see ResNet)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m                     \u001b[0mold_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTASK_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m                     \u001b[0;31m# old_outputs = old_architecture(images)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                     \u001b[0;31m# old_outputs = old_bias(old_outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/owr/our_modifications/open_world/BiC.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, task)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marchitecture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_correction_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTASK_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/owr/our_modifications/open_world/BiC.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_task\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mnew_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_task\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# new classes for this task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_classes\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_classes\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQFn3bM0R8Cn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}